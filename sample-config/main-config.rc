#!/bin/bash
#
# Unattended installer for OpenStack.
# Reynaldo R. Martinez P.
# E-Mail: TigerLinux@Gmail.com
#
# Main Configuration
# Version: 1.2.3.1.ub1604lts "Siberian Lynx"
# December 10, 2017
#

#
#
# SECURITY CONFIGURATION
#
# Our OpenStack automated installer applies a lot of IPTABLES rules in order
# to protect critical services from outside interference.
# The following variables should be set accordingly to your network:
#
# All openstack endpoints will be opened to the following IP or Network.
# Example:
# osprivatenetwork="192.168.0.0/16"
# If you are installing an AIO public server (on packet.net or another datacenter
# using public IP's) set the variable to the server IP. This will ensure that all
# your primary services will be closed on only reacheable from inside the server
# providing and extra measure of protection.
# If you are installing a multi-server setup with a controller, computes, and storage
# nodes, set the variable to your private network where all your openstack nodes are
# connected. Example: osprivatenetwork="192.168.56.0/24"
# Please, never ever use "0.0.0.0/0" here
osprivatenetwork="192.168.56.62"
#
# Keystone is already exposed to the IP's or Network configured on the "osprivatenetwork"
# variable, but, you can expose all keystone endpoints (ports 5000 and 35357) to an
# additional admin network, or even "0.0.0.0/0" if you want your Keystone to be reacheable
# from all the world.
#
keystoneclientnetwork="192.168.56.62"
#
# Manila and Designate are already exposed to any IP or Net included on the
# "osprivatenetwork" variable. If you want to expose both Manila and Designate services
# to a specific/additional network, IP, or to all the world, set the following variables:
#
manilaclientnetwork="0.0.0.0/0"
designateclientnetwork="0.0.0.0/0"
#
# Finally, you normally want to expose Horizon to all your nets, but if you want to specicy
# a single specific administrative network, set the variable to your desired source net:
horizonclientnetwork="0.0.0.0/0"


#
# DATABASE SECTION
#
# Choose the Database Flavor. By the moment, we only support mysql and postgres
#
# dbflavor = mysql | postgres
dbflavor="mysql"
#
# The following variable tell the installer if it should create the databases for
# all openstack services. If "dbcreate" is "yes", you must supply the root user and
# it's password. The installer asumes that no database software is installed, so the
# installer will take care of this task ONLY if the variable "dbinstall" is "yes".
# If you have access to an external database server, dbinstall should be "no".
# dbcreate="yes": All openstack related databases will be created in the selected
# database backend using the supplied user/password access to the server.
# dbinstall="yes": The installer will install the database software. If you use this
# option in "yes", also you MUST use "dbcreate=yes" and "dbpopulate=yes".
dbcreate="yes"
dbinstall="yes"
#
#
# NOTE !!.. ALERT !!... ALERT !!... ALERT !!!
# See the following bug: https://bugs.launchpad.net/glance/+bug/1672778
# If in your database password you do use %, @, ;, : or other special characters,
# SQLAlchemy (at least on Glance) will fail to provision and access the databases
# For the moment, and until the bug is solved, please set all database password to
# any combination of alphanumeric characters but AVOID special characters !!.
#
#
# Access information for MySQL
mysqldbadm="root"
mysqldbpassword="0p3nSt4ck"
mysqldbport="3306"
#
# Access information for PostgreSQL
psqldbadm="postgres"
psqldbpassword="0p3nSt4ck"
psqldbport="5432"
#
#
# The following variable indicates the installer to "populate" the openstack
# databases (cinder, glance, keystone, etc.). If this is a new installation, wheter
# you use an external database server or not, you shoud use "yes" here.
# If this a reinstallation, use "no"
#
dbpopulate="yes"
# NOTA: If "dbpopulate" is "no", the "dbcreate/dbinstall" variables are overriden and
# no action is taken for database installation, creation or population.
#
# Here you define the database backend server IP address. You can use an IP or a FQDN:
dbbackendhost="192.168.56.62"
#
# DB Max connections. This parameter needs to be adjusted according to your available
# cores in your controller
# 64 cores = 2000
# 128 cores = 4000
# 256 cores = 8000
dbmaxcons="1000"

#
# MESSAGE BROKER SECTION
#
# You must install the message broker in the controller or in the controller-compute but
# never in a compute node.
# If this server is a pure controller or a controller-compute, the variable must be "yes",
# else, put "no".
messagebrokerinstall="yes"
# The IP or FQDN of the message broker (always fill this, no matter if you are installing
# a controller or a compute node).
messagebrokerhost="192.168.56.62"
#
# From MITAKA, we dropped QPID support. Our default is rabbitmq. Please DO NOT TRY to change
# the following variable, or your installation WILL BREAK.
#
brokerflavor="rabbitmq"
#
# Autentication information for the message broker
brokeruser="openstack"
brokerpass="0p3nSt4ck"
# Virtual Host - Only for RabbitMQ - Please leave our default: "/openstack"
# WARNING: Please always include the "/" at the beggining of the virtual host name
brokervhost="/openstack"

#
# OPENSTACK SERVICES DATABASE ACCESS INFORMATION
#
# Here we define the database name, database user and database user password.
# In normal conditions, you should only need to change the password
#
# Keystone:
keystonedbname="keystonedb"
keystonedbuser="keystonedbuser"
keystonedbpass="0p3nSt4ck"
# Glance
glancedbname="glancedb"
glancedbuser="glancedbuser"
glancedbpass="0p3nSt4ck"
# Cinder
cinderdbname="cinderdb"
cinderdbuser="cinderdbuser"
cinderdbpass="0p3nSt4ck"
# Neutron
neutrondbname="neutrondb"
neutrondbuser="neutrondbuser"
neutrondbpass="0p3nSt4ck"
# Nova
novadbname="novadb"
novaapidbname="novaapidb"
novacell0dbname="novadb_cell0"
novadbuser="novadbuser"
novadbpass="0p3nSt4ck"
# Horizon
horizondbname="horizondb"
horizondbuser="horizondbuser"
horizondbpass="0p3nSt4ck"
# Heat
heatdbname="heatdb"
heatdbuser="heatdbuser"
heatdbpass="0p3nSt4ck"
# Trove
trovedbname="trovedb"
trovedbuser="trovedbuser"
trovedbpass="0p3nSt4ck"
# Sahara
saharadbname="saharadb"
saharadbuser="saharadbuser"
saharadbpass="0p3nSt4ck"
# Aodh
aodhdbname="aodhdb"
aodhdbuser="aodhdbuser"
aodhdbpass="0p3nSt4ck"
# Gnocchi
gnocchidbname="gnocchidb"
gnocchidbuser="gnocchidbuser"
gnocchidbpass="0p3nSt4ck"
# Manila
maniladbname="maniladb"
maniladbuser="maniladbuser"
maniladbpass="0p3nSt4ck"
# Designate
designatedbname="designatedb"
designatedbpoolmanagerdb="designatepoolmanagerdb"
designatedbuser="designatedbuser"
designatedbpass="0p3nSt4ck"
# Magnum
magnumdbname="magnumdb"
magnumdbuser="magnumdbuser"
magnumdbpass="0p3nSt4ck"

#
# OPENSTACK KEYSTONE
#
# In the following section we define keystone installation options
# If this machine is the controller, keystoneinstall must be "yes". If
# you are installing a compute node, you must use "no".
#
keystoneinstall="yes"
#
# Either controller or compute, put the IP or FQDN for the keystone host,
# normally, the controller server.
keystonehost="192.168.56.62"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# keystone hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extrakeystonehosts=""
# Please regenerate the SERVICE_TOKEN value by using the command: "openssl rand -hex 10"
SERVICE_TOKEN="044cb8c9e4a00a418827"
#
# PLEASE PLEASE PLEASE Leave Keystone Options following as they are. If you change 
# the default protected config options, your OpenStack installation will be unusable 
# 
# PROTECTED KEYSTONE CONFIG OPTIONS - DO NOT CHANGE !!!
#
keystoneadminuser="admin"
keystoneservicename="keystone"
keystoneadmintenant="admin"
keystoneservicestenant="services"
keystonememberrole="_member_"
keystoneuserrole="user"
keystonedomain="default"
keystonereselleradminrole="ResellerAdmin"
keystone_admin_rc_file="/root/keystonerc_admin"
keystone_fulladmin_rc_file="/root/keystonerc_fulladmin"
#
# END OF PROTECTED KEYSTONE CONFIG OPTIONS 
# 
# Change the remaining following options to fit your cloud infraestructure needs 
#
keystoneadminuseremail="admin@localhost"
keystoneadminpass="0p3nSt4ck"
#
# Token Provider Flavor. Three options: fernet, pki or uuid. From Mitaka, fernet is the
# default.
keystonetokenflavor="fernet"
#
# Endpoint Region. This is used for all service endpoints in the cloud.
# Sample: MyFirstOpenStackSite
# If you don't know what to put here, stick with the default provided by the installer:
endpointsregion="RegionOne"
#
# Extra Tenants: Those are sample tenants. The initial password will be
# the tenant name, a dash, and the password supplied in the "extratenantbasepass" variable.
# Sample:
# extratenantbasepass=0p3nSt4ck
# tenant name: extra1
# resulting password: extra1-0p3nSt4ck
extratenantbasepass="0p3nSt4ck"
extratenants="extra1 extra2 extra3"
domainextratenants="localhost"
# NOTE: Those extra tenants will be added to the "_member_" and "user" roles. Also the domain
# is the same for all and it's defined in the domainextratenants variable. You can later
# change this in Horizon Dashboard.
#


#
# OPENSTACK SWIFT
#
# You can choose to install or not "swift" by using swiftinstall variable in "yes" or "no".
# NOTE: Swift is by itself almost as complex as openstack is. If you want to install it for
# testing purposes, then OK use "yes", but if you don't really plan to use it, please leave
# this in "no".
# Change the IP, password and e-mail accordingly to your installation scenario.
#
swiftinstall="no"
swifthost="192.168.56.62"
swiftuser="swift"
swiftsvce="swift"
swiftpass="0p3nSt4ck"
swiftemail="swift@localhost"
#
# SWIFT Workers. Fine tune your swift workers or let swift to auto-tune (auto setting):
swiftworkers="auto"
#
# Swift extra configuration
#
# PLEASE PLEASE PLEASE read Swift Manuals first in you intend to use it
# Also consult the following link: http://docs.openstack.org/developer/swift/deployment_guide.html
partition_power="5"
partition_min_hours="1"
replica_count="1"
swiftfirstzone="1"
swiftfirstregion="1"
#
# Note: The following device is the mount point name inside the /srv/node directory.
# This point must be mounted in /etc/fstab with acl and user_xattr: Sample:
# /dev/sdc1               /srv/node/d1            ext4    acl,user_xattr  0 0
# If the moint point is not available, swift installation will fail.
swiftdevice="d1"
#
# If the following variable is "yes", during swift installation, any content in the
# specified device will be erased. PLEASE USE WITH EXTREME CARE !!!.
cleanupdeviceatinstall="no"
# Same as the aforementioned "cleanupdeviceinstall" but this one applies only during
# the uninstall.
cleanupdeviceatuninstall="yes"
#
# If you want to use swift as a storage backend for Glance, use "yes" in the following
# variable. Our default is "no".
glance_use_swift="no"
#


#
# OPENSTACK GLANCE
#
# If you are installing a controller, and want to use glance, you must use "yes" in
# order to install glance. If it is a compute, use "no":
glanceinstall="yes"
# The host IP is normally the one of the controller and you MUST specify it either if
# you are installing the controller or a compute node. This is normally the controller
# IP address or FQDN if you prefer to use DNS. Also, remember to change the password and
# e-mail.
glancehost="192.168.56.62"
glanceuser="glance"
glancesvce="glance"
glancepass="0p3nSt4ck"
glanceemail="glance@localhost"
#
# Fine tune your glance workers. You can set this to "at least" the actual core number
# in your controller/glance-server.
glanceworkers="2"
#
# If you want, we can install the cirrus test images. They are very usefull in order to
# test OpenStack Cloud Services. Our default: yes
glancecirroscreate="yes"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# glance hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extraglancehosts=""


#
# OPENSTACK CINDER
#
# If you are installing a controller, and want to use cinder, you must use "yes" in
# order to install cinder. If it is a compute, use "no". If it's a cinder storage node,
# set the variable to "yes" too:
cinderinstall="yes"
# The host IP is normally the one of the controller and you MUST specify it either if
# you are installing the controller or a compute node. This is normally the controller
# IP address or FQDN if you prefer to use DNS. Also, remember to change the password and
# e-mail.
# Note that "cinderhost" IP/FQDN is the one of your controller and it must be the same
# configured in all your cinder hosts, because it represents the "cinder controller" IP.
cinderhost="192.168.56.62"
cinderuser="cinder"
cindersvce="cinder"
cindersvcev2="cinderv2"
cindersvcev3="cinderv3"
cinderpass="0p3nSt4ck"
cinderemail="cinder@localhost"
# If your cinder node is a storage host (not all-in-one, not a controller), set the
# IP of your host here. If you are installing an "all-in-one" or a "controller", the
# ip here must be the same as in "cinderhost" variable above. If this node is a dedicated
# storage node, the "cinderhost" variable must be the same as your cinder controller, and 
# the following variable must be set to the "specific storage node" you are configuring:
cindernodehost="192.168.56.62"
#
# Next, you can select your storage backends. Do this if you are installing a cinder
# storage solution (cinder server with storage, or cinder storage-only node).
#
# If you want to include the lvm backend, leave the following variable at "yes". Otherwise,
# put "no" here:
cinderconfiglvm="yes"
# Our default is to use iscsi. Put here the iscsi server IP. It can be the one of the
# controller, if you are using a LVM "cinder-volumes" inside the controller. Also, if you are
# configuring a dedicated storage node, set the node IP here (not the one of your controller)
cinder_iscsi_ip_address="192.168.56.62"
cinderlvmname="cinder-volumes"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# cinder hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extracinderhosts=""
#
# The next variable controls if you want to clean-up all lv's at uninstall time.
# the default is "yes", meaning, all lv's in the cinder-volumes vg will be erased.
cindercleanatuninstall="yes"
#
# If you want to include nfs backend, change the following variable to "yes"
#
cinderconfignfs="no"
#
# Also, change the nfs resource for openstack. Tipically, an IP or FQDN followed by 
# ":" and the resource path
#
nfsresource="192.168.1.1:/data/openstack-nfs"
#
# Also, change the nfs mount options. The following should suit most configurations
# but again, change it and adapt it to your NFS environment
#
nfs_mount_options="rw,hard,intr,timeo=90,bg,vers=3,proto=tcp,rsize=32768,wsize=32768"
#
# Depending of your volume backend selection, set your default volume type. Our default is "lvm" in
# the first "cindernodehost" you configure. Due the fact that the installer let you config the same
# backend class in multiple cinder storage hosts, all backend classes are appended with the "node host
# IP address", so, if you have by example 3 hosts all with lvm enable, you'll end with 3 lvm's with
# the names: lvm-x.y.z.k, lvm-a.b.c.d and lvm-p.q.r.s so enable one as your default !.
default_volume_type="lvm-$cindernodehost"
#
# Finally, the most important setting for your cinder installation. You can install multiple
# cinder nodes with our installer. Your first note should be "at least" a "controller" node. Here,
# you are presented with 3 options:
# "allinone": This is a cinder node which is both a cinder-controller and a cinder-storage node.
# "controller": This is a cinder node which is a dedicated controller.
# "storage": This is a conder node which is a storage node.
# The default for us, if you are installing a "all-in-one" openstack server, is "allinone":
cindernodetype="allinone"
# Please remember: Your first cinder node must be either "allinone" or "controller". Aditional
# nodes must be set to "storage".
#
# Cinder volume clearing mode. Set to "none" or "zero". With "zero", cinder will erase it's volumes
# at deletion time using a "zero". Set to "none" to just delete the volume without clearing it.
cindervolclearmode="zero"
# If you set "cindervolclearmode" to "zero", set how many of the first megabytes to clear. Setting this
# variable to "0" will clear all the volume (this may take a lot of time). Our default is 50 (first 50
# megabytes).
cinderclearmb="50"
#



#
# OPENSTACK NEUTRON
#
# You must install neutron. If you are installing a controller node with network
# node (full controller), or a compute node, or a single neutron node, you must
# use "yes" here.
neutroninstall="yes"
#
#
# If the node you are installing is a neutron node or a controller with network
# node, the following variable must be set to "no". IF you are installing a single
# compute node, the variable must be set to "yes".
# The "yes" option ensures that the minimal neutron/openvswitch support needed for
# a compute node is installed along nova compute services.
# In othe words:
# Controller, All-in-One, or Network Node: "no"
# Nova Compute Node: "yes"
neutron_in_compute_node="no"
#
# The IP or FQDN of your neutron network node or controller
neutronhost="192.168.56.62"
#
# If you are installing a compute node, the following variable must be set to the
# server IP address (the server you are just installing). If you are installing a
# neutron network node or an ALL-IN-ONE server, use the same IP of the neutronhost
# variable.
neutron_computehost="192.168.56.62"
#
# Here, you should chang the password and e-mail
neutronuser="neutron"
neutronsvce="neutron"
neutronpass="0p3nSt4ck"
neutronemail="neutron@localhost"
#
# Finetune your neutron API workers here:
neutronapiworkers="2"
#
# This is the most important part of your network configuration. Here, you must
# select your network mappings. Please, ensure you understand OpenStack
# network philosophy before you configure this.
# Our defaults assumes a flat network configuration with external routers connected
# to an openvswitch br-eth2 switch. Change those defaults to suit your network or your
# system will not have a viable network service.
integration_bridge="br-int"
# The bridge_mappings must be set in the form:
# physical_network_name01:bridge_interface01,physical_network_name02:bridge_interface02
# Sample:
# bridge_mappings="physical01:br-eth1,physical02:br-enp0s9,physical03:br-em2"
# VERY IMPORTANT NOTE: Ensure your OVS Bridges are created before you install openstack,
# including br-int and all bridges defined in the "bridge_mappings" variable. Fail
# to do this, and Neutron will be unable to function properlly.
bridge_mappings="physical01:br-enp0s9"
dhcp_domain="localdomain"
metadata_shared_secret="0p3nSt4ck"
# VM's Base Mac. Normally you should not need to change this
basemacspec="fa:16:3e:00:00:00"
# Lease duration in seconds
# "-1" set "infinite" lease duration
dhcp_lease_duration="-1"
#
# If you change the following variable to "yes", we'll include a DNSMASQ rule
# that will try to force a MTU of 1454 bytes trough dhcp option 26. This is only
# needed if you are going to use GRE. If you are using vlan of flat networks only, let
# the option in "no"
forcegremtu="no"
#
# Tenant network type. Set this either to "gre" or "vxlan". Our default is "gre". Please
# note that, for proper performance with vxlan, your network should use Jumbo Frames.
tenant_network_types="gre"
# NOTE: You can also set other tenant typs like local, flat or even se to multiple options
# like: tenant_network_types="flat,vlan,gre,vxlan", but, this can bring some admin
# problems and, specially if you plan to use MAGNUM, there will be no way to let magnun
# to create it's container private networks with a specific type. Then, our recommendation
# is to stick with either "gre" or "vxlan".
#
# If you are going to use gre or vxlan, change the following variables according
# to your preferences
tunnel_id_ranges="1:100"
vni_ranges="110:1000"
#
#
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# neutron hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extraneutronhosts=""
#
# We use a customized configuration file for dnsmasq (the OpenStack DHCP Service). See
# our readme in order to understand what you can do with this file.
dnsmasq_config_file="/etc/dnsmasq-neutron.conf"
#
# Neutron DHCP Agent redundancy:
# By default, we only install a single dhcp-agent in the controller/neutron-server. If
# you want, you can install an additional dhcp_agent in a compute node. If you want to
# install an extra dhcp agent and this config file belongs to a compute node, please
# change "dhcp_agents_in_compute_node" to "yes" and "dhcp_agents_per_network" to "2".
# If you don't want or don't need such redundancy, leave the following two lines as they
# are (no and 1 respectivelly).
dhcp_agents_in_compute_node="no"
dhcp_agents_per_network="1"
#
# If you want our installer to automatically create the neutron FLAT networks, you can
# use "yes" in flat_network_create and list the networks, space separated, in the create list
# variable.
# every network item must be in the form: physical-net:logical-net. Physical-net must be
# in your bridge_mappings above (se the sample: physical01:br-eth2)
# Sample:
# flat_network_create_list="physical01:public physical01:private physical02:external physical02:internal"
flat_network_create="no"
flat_network_create_list=""
# Also, you need to set up your physical FLAT network names. Sample:
# flat_networks="physical01,physical02"
flat_networks="physical01"
#
#
#
# If you want our installer to automatically create the neutron VLAN networks, you can
# use "yes" in vlan_network_create and list the networks, space separated, in the create list
# variable.
# every network item must be in the form: physical-net:logical-net:vlan_id. Physical-net must be
# in your bridge_mappings above (se the sample: physical01:br-eth2). The "vlan_id" is your vlan tag.
# Sample:
# vlan_network_create_list="physical01:public:16 physical01:private:120 physical02:external:200"
vlan_network_create="no"
vlan_network_create_list=""
# Also, you need to set up your network vlan ranges. Sample:
# network_vlan_ranges="physical01:1:20,physical01:20:200,physical02:100:300"
network_vlan_ranges=""
#
# If you want to use VPN As a Service, set the following to "yes"
vpnaasinstall="no"
#
# If you want to use the neutron metering module, set this to yes (our default)
neutronmetering="yes"



#
# OPENSTACK NOVA
#
# If you are installing a controller, nova control node, or nova compute, you must set
# this to yes:
novainstall="yes"
#
# If the node you are installing is a full controller, ALL-IN-ONE, or a nova controll node,
# the following variable must be set to "no". If you are installing a compute node, set the
# variable to "yes".
# This will set the installer to select the nova needed packages and nothing extra.
# In other words:
# Controller, nova control node or All-In-One: "no"
# Nova Compute Node: "yes"
nova_in_compute_node="no"
#
# Also, if you plan to install a pure-controller (nova without compute service), or a pure
# nova control node, you must set this variable to "yes". This will ensure that no Virtual Machines
# will be deployed in the server. Of course, if you are installing an ALL-IN-ONE, you must leave
# the "no" default.
#
# The "yes" option just instruct the installer to "not install" the compute service dependencies.
#
nova_without_compute="no"
#
# This is the IP of your nova control node, controller or ALL-IN-ONE server.
novahost="192.168.56.62"
#
# If you are installing NOVA in a compute node, the following IP must be ser to the server IP. If you
# are installing a nova control node, all-in-one or controller, the IP must be set as the same in
# "novahost" variable.
nova_computehost="192.168.56.62"
#
# Here you normally only need to change the password and e-mail:
novauser="nova"
novaplacementuser="placement"
novasvce="nova"
novaplacementsvce="placement"
novaec2svce="ec2"
novapass="0p3nSt4ck"
novaplacementuserpass="0p3nSt4ck"
novaemail="nova@localhost"
novaplacementemail="placement@localhost"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# nova hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extranovahosts=""
#
# Nova API Workers to launch. Fine tune your API workers here.
osapiworkers="2"
# Nova CONDUCTOR Workers to launch. Fine tune your conductor workers here.
conductorworkers="2"
#
# Over-subscription CPU/RAM/DISK control. Please read in openstack manuals what this mean.
# You probably want to use those defaults... then again, maybe not.
ram_allocation_ratio="1.5"
cpu_allocation_ratio="16.0"
disk_allocation_ratio="1.0"
#
# The same way we allow you to select flavors for database, token provider and message
# broker, we also can allow you to select the console flavor. The default (vnc) is OK for
# most situations, but if you want you can choose spice.
# Options: vnc or spice. Default: vnc
consoleflavor="vnc"
# Spice Server: This is the IP of our controller, ALL-IN-ONE, or nova control node:
spiceserver_controller_address="192.168.56.62"
# VNC Server: This is the IP of our controller, ALL-IN-ONE, or nova control node:
vncserver_controller_address="192.168.56.62"
# Keymap: User "us" or "es" here
vnc_keymap="es"
#
# If "yes", our installer will create the nova/neutron security groups for icmp, ssh and rdp access
vm_default_access="yes"
# Also, we can create extra access for other tcp and udp ports
vm_extra_ports_tcp="25 53 80 110 143 443"
vm_extra_ports_udp="53"
#
# Leave this option as its default if you want to allow resize in the same host.
allow_resize_to_same_host="True"
#
# Those variables indicates nova to auto-start VM's on compute start, and also resume last VM's
# states if possible. You REALLY want this on "False", but if you know what are you doing, use "True".
start_guests_on_host_boot="False"
resume_guests_state_on_host_boot="False"
#
# Instance name template.
instance_name_template="instance-%08x"
#
# CPU Mode (only valid for kvm). Normally, the best performance is with host-passthrough. See the
# following link for more information:
# https://wiki.openstack.org/wiki/LibvirtXMLCPUModel
# Common values: host-passtrough or host-model
libvirt_cpu_mode="host-passthrough"
#
#
# Normally, our installer autodetects if KVM is available and possible, but, in some rare circunstances
# this auto-detection can fail and enable kvm where is not really possible, or convenient to use. In
# such cases, you can force qemu instead of kvm. Be aware that kvm is the best option on physical hardware
# where the cpu and motherboard supports intel/amd hardware virtualization. If you are using a vm inside
# vmware, kvm-qemu, openstack or another VPS that is already virtual, you "should" change the following
# variable to "yes". Otherwise, leave it to "no".
forceqemu="no"
#
# VHOST NET: Normally, this is a good option to set in your system. If you leave this to our default (yes)
# we'll force vhost_net module at boot time into the operating system:
vhostnet="yes"



#
# OPENSTACK CEILOMETER
#
# If you are installing a controller, ALL-IN-ONE or compute node, and you want to
# include metrics in your cloud, set this to yes. Also, if you plan to use HEAT with
# AutoScaling, YOU MUST set this to yes:
ceilometerinstall="yes"
#
# If you are installing a full ceilometer node, or a pure controller (no compute service),
# set the following variable to "no".
# If yoy are installing a compute node, or an ALL-IN-ONE, set the variable to yes.
#
# In other words:
# Controller, All-In-One or Ceilometer Node without compute: "no"
# Compute node: "yes"
ceilometer_in_compute_node="no"
#
# Also, if you want to install a pure controller (no compute node) or a pure ceilometer,
# node, user "yes". In normal conditions, leave it at "no".
#
ceilometer_without_compute="no"
#
# This is the IP of the ceilometer node host or controller. It is the same for all 3 entries:
ceilometerhost="192.168.56.62"
aodhhost="192.168.56.62"
gnocchihost="192.168.56.62"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# aodh hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extraaodhhosts=""
# Change the password and e-mail (ceilometer and gnocchi)
ceilometeruser="ceilometer"
ceilometersvce="ceilometer"
ceilometerpass="0p3nSt4ck"
ceilometeremail="ceilometer@localhost"
gnocchiuser="gnocchi"
gnocchisvce="gnocchi"
gnocchipass="0p3nSt4ck"
gnocchiemail="gnocchi@localhost"
# Same for alarming service, change the password and e-mail
aodhuser="aodh"
aodhsvce="aodh"
aodhpass="0p3nSt4ck"
aodhemail="aodh@localhost"
#
# MongoDB access information. We recommend to leave this as it is.. Just change the IP.
# The IP should be the one where the ceilometer node or controller is installed.
mondbhost="192.168.56.62"
mondbport="27017"
mondbname="ceilometer"
mondbuser="ceilometer"
# NOTE: For some reazon, the MongoDB password cannot contain "@" or similar characters which
# need to be "escaped". Please just to be sure no problems arise here, use a password with
# normal ascii and numbers. Use the following as a guide.
mondbpass="C3iL0m3T3r-Op3nStaCk"
# Metering secret. Please regen this by using the following command:
# openssl rand -hex 10
metering_secret="fe01a6ed3e04c4be1cd8"
#
# If you want ceilometer alarms, leave this on "yes". Case else, use "no". Please note that
# Heat with AutoScaling depends on ceilometer alarms. So, leave this to yes if you plan to
# use such feature.
ceilometeralarms="yes"
#
# If you install swift, and want to include swift metrics in ceilometer, set the following
# variable to yes. Otherwise, leave it to "no"
#
swiftmetrics="no"
#
# Ceilometer Time To Live data (in seconds)
# The following value controls the MongoDB data expiration in ceilometer
# The proposed value here amounts for 7 days (3600x24x7)
# NOTE: From release 15, we are using Gnocchi. Mongo is not being installed any more so this
# setting is a "legacy" and it will be deprecated soon !
#
mongodbttl="604800"
#
# Measurement interval. By default, ceilometer take measurements every 600 seconds, but, if you
# plan to include AutoScaling in your cloud, is better idea to set the value to 60 seconds. Let
# the following value in 60 seconds unless you don't plan to use AutoScaling. In that case, set
# the value to 600 seconds.
ceilointerval="60"
#
# Workers for ceilometer, aodh and gnocchi
ceilometerworkers="2"
aodhworkers="2"
gnocchiworkers="2"



#
# OPENSTACK HEAT
#
# Install heat on the controller only. Our default is "no", but set the following variable to "yes"
# if you don't want to install heat. Again: IN THE CONTROLLER !!!
heatinstall="yes"
# The heat host IP: Normally, the controller IP
heathost="192.168.56.62"
# Change the password and email
heatuser="heat"
heatsvce="heat"
heatpass="0p3nSt4ck"
heatemail="heat@localhost"
heatcfnsvce="heat-cfn"
stack_domain_admin="stack_domain_admin"
stack_domain_admin_password="0p3nSt4ck"
stack_user_domain_name="heat"
heat_stack_user_role="heat_stack_user"
heat_stack_owner="heat_stack_owner"
# Regen the following variable by using: "openssl rand -hex 32"
# NOTE: For heat, the encription key MUST BE 32 long. Use -hex 32 ALWAYS !!
# in order to regenerate the key.
heatencriptionkey="7a6502a023ff8a3a16fbc2e438bc7ddfd524707cc892c30769f7411b68de423b"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# heat hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extraheathosts=""
#
# HEAT Workers. Fine tune your heatservice workers here:
heatworkers="2"



#
# OPENSTACK TROVE
#
# Our default is to not install trove, but if you want to installit, set this to "yes". ONLY
# IN THE CONTROLLER
troveinstall="no"
# The controller IP of course.... ;)
trovehost="192.168.56.62"
# As usual, change the password and e-mail.
troveuser="trove"
trovesvce="trove"
trovepass="0p3nSt4ck"
troveemail="trove@localhost"
# If you want to use CINDER as Database storage backend, set the following variable to "yes".
# if you let the variable as "no" (our default), trove will use locally base storage. This
# mean you should use a flavor with ephemeral disk along root and swap disk. Trove configures
# the database space in the extra disk (does not use the root disk for database storage):
trovevolsupport="no"
# Also, ensure the proper volume device (our default is vdb, considering a normal instance
# with vda as primary disk, vdb as ephemeral disk and vdc as swap disk). If you are using
# trove instances with local-based storage, the flavor must contain an ephemeral disk and
# swap disk. In this case, trovevoldevice must be vdb. If you plan to use trove with cinder
# based storage, the root disk will be vda, the swap disk will be vdb, and the cinder disk
# will take vdc. In this case, trovevoldevice must be vdc and your flavor should not contain
# an ephemeral disk. Just root and swap.
trovevoldevice="vdb"
# Trove default DataStore - Normally, mysql. Se trove documentation for other options, as trove
# support many database engines:
trovedefaultds="mysql"
#
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# trove hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extratrovehosts=""
#
# IMPORTANT NOTE:
# To make your life easier, this installer creates a sample trove-guestagent config file
# already prepared to work with your OpenStack environment. You'll find this at:
# /etc/trove/trove-guestagent.conf
# Use this config file in your trove datastore-related glance images !!. The file is prepared
# wot work with your default datastore (see "trovedefaultds" variable above) but you can adapt
# it to other datastores.
#
# NOTE about backups and replicas: Trove requires swift for backups and replicas. If you
# want trove to make your db backups, and/or want/or/need to use replicas, then swift is
# a mandatory component. Ensure to set "swiftinstall" to "yes" if you opt to install trove
# and need backups and/or replicas.
#
# TROVE Workers. Fine tune your trove service workers here:
troveworkers="2"




#
# OPENSTACK SAHARA
#
# Same as with heat and trove. By default we don't install them. If you want to include sahara
# in your cloud, set this yo YES. And again, install this in the controller or in dedicated
# sahara host
saharainstall="no"
# The controller IP or the IP of the dedicated sahara host.
saharahost="192.168.56.62"
# Again, change the password and e-mail
saharauser="sahara"
saharasvce="sahara"
saharapass="0p3nSt4ck"
saharaemail="sahara@localhost"
#
# The next variable controls if you want to clean-up all lv's at uninstall time.
# the default is "yes", meaning, all lv's in the manila-volumes vg will be erased.
manilacleanatuninstall="yes"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# sahara hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extrasaharahosts=""



#
# OPENSTACK MANILA - ALERT ALERT ALERT: Installing Manila or Designate on Ubuntu 16.04 WILL 
# BREAK Horizon !!!. DO NOT install it on productions environment !!!
# There is an apparent bug that makes unusable Horizon when Manila or Designate is installed
#
# Same as with heat,trove and sahara. By default we don't install them. If you want to include
# manila in your cloud, set this yo YES. And again, install this in the controller or in a 
# dedicated manila host
manilainstall="no"
# The controller IP or the IP of the dedicated manila host.
manilahost="192.168.56.62"
# Again, change the password and e-mail
manilauser="manila"
manilasvcev1="manila"
manilasvcev2="manilav2"
manilapass="0p3nSt4ck"
manilaemail="manila@localhost"
#
# We can configure the LVM based VG for you. This requires an already present 
# manila-dedicated volume group.
manilalvmbackend="yes"
# The Manila VG Name - This must be already created or your manila installation
# will fail. Note: By the moment, we only support LVM based backends for Manila,
# but you can still manually configure other backends.
manilavg="manila-volumes"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# manila hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extramanilahosts=""



#
# OPENSTACK DESIGNATE - EXPERIMENTAL
# ALERT ALERT ALERT: Installing Manila or Designate on Ubuntu 16.04 WILL BREAK Horizon !!!.
# DO NOT install it on productions environment !!!
# There is an apparent bug that makes unusable Horizon when Manila or Designate is installed
#
# We are including EXPERIMENTAL support for Designate. Please, read all the documentation
# about OpenStack Designate in order to fully understand the way DNSaaS is implemented with
# designate.
# As with the other modules (trove, manila, etc.) our default is "no". Change it to "yes" in
# order to install designate:
designateinstall="no"
designatehost="192.168.56.62"
# Again, change the password and e-mail
designateuser="designate"
designatesvc="designate"
designatepass="0p3nSt4ck"
designateemail="designate@localhost"
#
# This is very important. Pur here a valid e-mail. This is the e-mail that will be configured
# by default inside your zone pool.
zonepooldefaultemail="designateadmin@localdomain.dom"
#
# Our installer provision designate TLD's from the file at: 
# http://data.iana.org/TLD/tlds-alpha-by-domain.txt
# but, you can add extra TLD's (normally not "legal" tld's) that you would like to define
# inside your organization. Note that you cannot create domains whose tld's are not defined
# into designate:
dnsextratlds="home dom bogus"
dnsextratldfile="/etc/designate/extra-tlds.txt"
#
# Optional feature: If you want, we can configure designate SINK service in order to
# auto-create DNS records in an specific DNS domain (designate managed)
# For this feature to work correctly, please choose "yes" in the following variable
# and supply the DNS domain that we will auto-create in Designate and configure in the
# SINK service:
dnssinkactivate="no"
dnssinkdomain="mydnsdomain.org"
dnssinkdomainemail="designateadmin@mydnsdomain.org"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# designate hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extradesignatehosts=""



#
# OPENSTACK MAGMUN
#
# Install magnum on the controller only. Our default is "no", but set the following
# variable to "yes" if you want to install magnum. Again: IN THE CONTROLLER !!!
magnuminstall="no"
#
# VERY IMPORTANT NOTE: Magnum REQUIRES HEAT !. It WONT WORK without heat, so, set
# "heatinstall" to "yes". Also, if you plan to deploy docker or kubernetes container
# clusters, you will need persistent storage (Cinder with storage configured and
# fully available). Also, if you plan to create your cluster templates with local
# registry, you'll need "swift" too.
#
# The magnum host IP: Normally, the controller IP
magnumhost="192.168.56.62"
# Change the password and email
magnumuser="magnum"
magnumsvce="magnum"
magnumpass="0p3nSt4ck"
magnumemail="magnum@localhost"
magnum_domain_name="magnum"
magnum_domain_admin="magnum_domain_admin"
magnum_domain_admin_password="0p3nSt4ck"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# magnum hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extramagnumhosts=""
#
# Fine tune your magnum API workers here:
magnumworkers="2"



#
# SNMP MONITORING SUPPORT
#
# If you want to include snmpd monitoring, set the following variable to "yes". We include
# some MIB's that will help you to monitor you cloud.
#
# The default community we install (if no previous snmpd.conf file present) is "OpenStack"
# (Capital "O" and "S").
# See /etc/snmp/snmpd.conf for more information
#
snmpinstall="no"



#
# OPENSTACK HORIZON
#
# NOTE: Here, all "slashes" must go escaped
# You MUST include horizon in your installation. Install it (set yes in the following variable)
# in your Controller Node, or ALL-IN-ONE. It is also possible to select an horizon node also if
# you want.
horizoninstall="yes"
# By default, we are using the HIGHLY-RECOMMENDED-PLEASE-DON'T-CHANGE-IT database based cache.
horizondbusage="yes"
# Our timezone. Whatever timezone you use, please remember to escape the slash
dashboard_timezone="America\/Caracas"
# Your horizon server IP - normally the controller IP.
horizonhost="192.168.56.62"
# Only if you are using MySQL as DB backend. If for any chance, you have multiple
# horizon hosts, put here the IP address of the extra hosts, space separated.
# Normally, this is not necessary unless your mysql service access policies are very
# strict.
extrahorizonhosts=""
# If you don't use the database, we'll use the "PLEASE-DONT-USE-THIS" option of memcached
memcached_spec="memcached:\/\/127.0.0.1:11211\/"
# Finally, the branding name. Set as your personal or corporate preference
brandingname="OpenStack Private Cloud"

#
# END OF THE CONFIGURATION FILE
#
